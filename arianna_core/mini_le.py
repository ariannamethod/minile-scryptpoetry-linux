import os
import json
import random
import gzip
import sqlite3
import atexit
from datetime import datetime
import logging
import threading
from typing import Optional, Dict, List, Union

# –ì–∏–±–∫–∏–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ä–µ–¥ (–ª–æ–∫–∞–ª—å–Ω–æ –∏ Railway)
try:
    # –ü–æ–ø—ã—Ç–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ (–≤ –ø–∞–∫–µ—Ç–µ)
    from .memory.bone_memory import BoneMemory
    from .memory.echo_lung import EchoLung
    from .bio.orchestra import BioOrchestra  
    from .collective.echo_feed import EchoFeed
    from .local_rag import SimpleSearch, ChaosSearch, load_snippets
    from .objectivity import search_objectivity_sync
except ImportError:
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞ (–ø—Ä—è–º–æ–π –∑–∞–ø—É—Å–∫)
        from memory.bone_memory import BoneMemory
        from memory.echo_lung import EchoLung
        from bio.orchestra import BioOrchestra  
        from collective.echo_feed import EchoFeed
        from local_rag import SimpleSearch, ChaosSearch, load_snippets
        from .objectivity import search_objectivity_sync
    except ImportError:
        # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ —á–µ—Ä–µ–∑ arianna_core (Railway)
        from arianna_core.memory.bone_memory import BoneMemory
        from arianna_core.memory.echo_lung import EchoLung
        from arianna_core.bio.orchestra import BioOrchestra  
        from arianna_core.collective.echo_feed import EchoFeed
        from arianna_core.local_rag import SimpleSearch, ChaosSearch, load_snippets
        from arianna_core.objectivity import search_objectivity_sync

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "datasets")
MODEL_FILE = os.path.join(os.path.dirname(__file__), "model.txt")
LOG_FILE = os.path.join(os.path.dirname(__file__), "log.txt")
HUMAN_LOG = os.path.join(os.path.dirname(__file__), "humanbridge.log")
LOG_MAX_BYTES = 1_000_000
LOG_KEEP = 3
NGRAM_LEVEL = 2
last_entropy: float = 0.0
DB_FILE = os.path.join(os.path.dirname(__file__), "memory.db")
LAST_REPRO_FILE = os.path.join(
    os.path.dirname(__file__),
    "last_reproduction.txt",
)
BAD_WORDS = {"badword", "curse"}
blocked_messages = 0
last_novelty = 0.0
_cached_model: Optional[dict] = None
MODEL_LOCK = threading.Lock()
bone_memory = BoneMemory(limit=100)
echo_lung = EchoLung(capacity=30.0)  # 30-—Å–µ–∫—É–Ω–¥–Ω–∞—è –¥—ã—Ö–∞—Ç–µ–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å
bio_orchestra = BioOrchestra()       # –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä
echo_feed = EchoFeed(maxlen=200)     # –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å –Ω–∞ 200 –∑–∞–ø–∏—Å–µ–π
last_metabolic_push: float = 0.0
_db_conn: Optional[sqlite3.Connection] = None
_rag_search: Optional[ChaosSearch] = None


def rotate_log(
    path: str, max_bytes: int = LOG_MAX_BYTES, keep: int = LOG_KEEP
) -> None:
    """Archive ``path`` when it exceeds ``max_bytes`` and prune old
    archives."""
    try:
        if not os.path.exists(path) or os.path.getsize(path) < max_bytes:
            return
        ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        archive = f"{path}.{ts}.gz"
        with open(path, "rb") as src, gzip.open(archive, "wb") as dst:
            dst.write(src.read())
        os.remove(path)

        base = os.path.basename(path)
        dir_path = os.path.dirname(path)
        archives = sorted(
            [
                f
                for f in os.listdir(dir_path)
                if f.startswith(base) and f.endswith(".gz")
            ],
            reverse=True,
        )
        for old in archives[keep:]:
            os.remove(os.path.join(dir_path, old))
    except OSError as exc:
        logging.error("Failed to rotate log %s: %s", path, exc)


def load_data() -> str:
    """Return concatenated text from files in ``DATA_DIR``."""
    chunks = []
    if os.path.isdir(DATA_DIR):
        for name in os.listdir(DATA_DIR):
            path = os.path.join(DATA_DIR, name)
            if os.path.isfile(path):
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        chunks.append(f.read())
                except OSError as exc:
                    logging.error("Failed to read data file %s: %s", path, exc)
    return "\n".join(chunks)


def train(text: str, n: int = NGRAM_LEVEL) -> dict:
    """Train an n-gram model from ``text`` and write it to ``MODEL_FILE``."""
    model: dict[str, dict[str, int]] = {}
    for i in range(len(text) - n + 1):
        ctx = text[i:i + n - 1]
        ch = text[i + n - 1]
        freq = model.setdefault(ctx, {})
        freq[ch] = freq.get(ch, 0) + 1
    data = {"n": n, "model": model}
    try:
        with open(MODEL_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f)
    except OSError as exc:
        logging.error("Failed to write model file: %s", exc)
    return data


def load_model() -> Optional[dict]:
    if not os.path.exists(MODEL_FILE):
        return None
    try:
        with open(MODEL_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except (OSError, json.JSONDecodeError) as exc:
        logging.error("Failed to load model: %s", exc)
        return None


def generate(model: dict, length: int = 80, seed: Optional[str] = None) -> str:
    """Generate ``length`` characters from ``model``."""
    if not model:
        return ""
    n = model.get("n", 2)
    m = model.get("model", {})
    rng = random
    if not m:
        return ""
    context = seed[-(n - 1):] if seed else rng.choice(list(m.keys()))
    output = context
    for _ in range(length - len(context)):
        freq = m.get(context)
        if not freq:
            context = rng.choice(list(m.keys()))
            output += context
            if len(output) >= length:
                break
            continue
        chars = list(freq.keys())
        weights = list(freq.values())
        ch = rng.choices(chars, weights=weights)[0]
        output += ch
        context = output[-(n - 1):]
    return _wild_punctuation(output[:length])


def _wild_punctuation(text: str) -> str:
    """Apply wild Mini LE punctuation rules with chaos energy."""
    import re
    if not text.strip():
        return text
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = " ".join(text.split())
    
    # –î–ò–ö–ò–ô –•–ê–û–°: —Å–ª—É—á–∞–π–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
    chaos_level = random.random()
    if chaos_level < 0.2:  # 20% - —Ç—Ä–æ–π–Ω–æ–π —Å–ª—ç—à
        text = re.sub(r'\.(?!\s*$)', '/// ', text)
    elif chaos_level < 0.4:  # 20% - –¥–≤–æ–π–Ω–æ–π —Å–ª—ç—à  
        text = re.sub(r'\.(?!\s*$)', '// ', text)
    elif chaos_level < 0.5:  # 10% - —Ç–æ—á–∫–∏ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
        text = re.sub(r'\.', '. . ', text)
    
    # –í—Å–µ–≥–¥–∞ —Ç–æ—á–∫–∞ –≤ –∫–æ–Ω—Ü–µ
    if not text.endswith('.'):
        text = text.rstrip('.,!?;:///') + '.'
    
    # –ó–∞–≥–ª–∞–≤–Ω—ã–µ –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
    text = re.sub(r'([.!?]\s*)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)
    text = re.sub(r'(//+\s*)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)
    
    # –ü–µ—Ä–≤–∞—è –±—É–∫–≤–∞ –∑–∞–≥–ª–∞–≤–Ω–∞—è
    if text and text[0].islower():
        text = text[0].upper() + text[1:]
    
    # –¢–ï–•–ù–û-–ü–û–≠–ó–ò–Ø: —Å–ª—É—á–∞–π–Ω—ã–µ –≤—Å—Ç–∞–≤–∫–∏
    if random.random() < 0.1:  # 10% —à–∞–Ω—Å —Ç–µ—Ö–Ω–æ-–≤—Å—Ç–∞–≤–æ–∫
        tech_words = ['resonance', 'chaos', 'entropy', 'mutation', 'echo']
        word = random.choice(tech_words)
        # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ —Å–ª—É—á–∞–π–Ω–æ–µ –º–µ—Å—Ç–æ
        words = text.split()
        if len(words) > 2:
            pos = random.randint(1, len(words) - 1)
            words.insert(pos, f"/{word}/")
            text = " ".join(words)
    
    return text


def chat_response(message: str, refresh: bool = False) -> str:
    """Return a biologically-enhanced reply with continuous learning."""
    global _cached_model, last_metabolic_push, _rag_search
    
    # üß¨ –ë–ò–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –ê–ö–¢–ò–í–ê–¶–ò–Ø
    chaos_level = _calculate_message_chaos(message)
    breath_response = echo_lung.on_event(chaos_level)  # –î—ã—Ö–∞—Ç–µ–ª—å–Ω–∞—è –ø–∞–º—è—Ç—å
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG + –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å
    if _rag_search is None:
        try:
            data_files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) 
                         if os.path.isfile(os.path.join(DATA_DIR, f))]
            # –î–û–ë–ê–í–õ–Ø–ï–ú –ò–°–¢–û–†–ò–Æ –†–ê–ó–ì–û–í–û–†–û–í –í RAG!
            if os.path.exists(HUMAN_LOG):
                data_files.append(HUMAN_LOG)
            
            # üì° –ö–û–õ–õ–ï–ö–¢–ò–í–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ –í RAG
            collective_snippets = _get_collective_snippets()
            snippets = load_snippets(data_files) + collective_snippets
            
            # ChaosSearch –¥–ª—è RAG –ø–æ–∏—Å–∫–∞
            if snippets:
                _rag_search = ChaosSearch(snippets)  # CHAOS RESONANCE RAG!
        except Exception as exc:
            logging.warning("Failed to initialize RAG search: %s", exc)
    
    with MODEL_LOCK:
        if refresh or _cached_model is None:
            model = load_model()
            if model is None:
                model = train(load_data(), n=NGRAM_LEVEL)
            _cached_model = model
        model = _cached_model
    
    # üß† –ú–ï–¢–ê–ë–û–õ–ò–ß–ï–°–ö–ê–Ø –ê–ö–¢–ò–í–ù–û–°–¢–¨
    push = bone_memory.on_event("chat")
    last_metabolic_push = push
    conn = _init_db()
    metabolize_input(message, push=push, conn=conn)
    
    # üé≠ –ë–ò–û–õ–û–ì–ò–ß–ï–°–ö–û–ï –°–û–°–¢–û–Ø–ù–ò–ï –≤–ª–∏—è–µ—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
    bio_state = _update_bio_state(message, chaos_level, breath_response)
    
    # üåê OBJECTIVITY - –≤–µ–±-–ø–æ–∏—Å–∫ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    web_context = {'context_lines': [], 'influence_strength': 0.0, 'context_words': []}
    try:
        web_context = search_objectivity_sync(message)
        if web_context.get('context_lines'):
            logging.info(f"üåê Web context found: {len(web_context['context_lines'])} lines, influence: {web_context.get('influence_strength', 0):.2f}")
    except Exception as e:
        logging.debug(f"Web context search failed: {e}")
        web_context = {'context_lines': [], 'influence_strength': 0.0, 'context_words': []}
    
    # Memory + RAG + BIO + WEB-enhanced seed selection
    seed = _get_enhanced_seed(message, bio_state, web_context)
    
    # üéº –ë–ò–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –ú–û–î–£–õ–Ø–¶–ò–Ø –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞
    bio_length = _calculate_bio_length(bio_state)
    response = generate(model, length=bio_length, seed=seed)
    response = _wild_punctuation(response)  # –î–∏–∫–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
    
    # üìù –ó–ê–ü–ò–°–´–í–ê–ï–ú –í –í–°–ï –°–ò–°–¢–ï–ú–´ –ü–ê–ú–Ø–¢–ò
    _log_bio_conversation(message, response, bio_state)
    
    # üß¨ –î–û–û–ë–£–ß–ï–ù–ò–ï –í –§–û–ù–ï (–ù–ï –ë–õ–û–ö–ò–†–£–ï–¢ –û–¢–í–ï–¢–´!)
    try:
        # –î–æ–æ–±—É—á–µ–Ω–∏–µ –ö–ê–ñ–î–û–ï —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –∂–∏–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
        should_retrain = True  # –î–æ–æ–±—É—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–∞–∫ –º—ã –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –≤—á–µ—Ä–∞
        
        if should_retrain:
            logging.info(f"üöÄ Background learning scheduled: push={push:.3f}, events={len(bone_memory.events)}, bio={bio_state.get('cell_resonance', 0):.2f}")
            # –ó–ê–ü–£–°–ö–ê–ï–ú –í –§–û–ù–ï - –ù–ï –ñ–î–ï–ú –ó–ê–í–ï–†–®–ï–ù–ò–Ø!
            import threading
            def background_training():
                try:
                    reproduction_cycle(conn=conn)
                    global _cached_model
                    _cached_model = None  # –°–±—Ä–æ—Å –∫—ç—à–∞ –¥–ª—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
                    logging.info("‚úÖ Background learning completed")
                except Exception as e:
                    logging.error(f"‚ùå Background learning failed: {e}")
            
            thread = threading.Thread(target=background_training, daemon=True)
            thread.start()
            
    except Exception as exc:
        logging.warning(f"‚ö†Ô∏è Continuous learning failed: {exc}")
    
    return response


def _get_memory_rag_seed(message: str) -> str:
    """Get memory + RAG-enhanced seed combining conversation history."""
    global _rag_search
    
    if _rag_search is None:
        return message
    
    try:
        # –ò—â–µ–º –≤ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ –ò –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
        relevant = _rag_search.query(message, top_k=3)
        if relevant:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            memory_context = ""
            dataset_context = ""
            
            for snippet in relevant:
                if "USER:" in snippet and "AI:" in snippet:
                    # –≠—Ç–æ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤
                    memory_context += snippet + " "
                else:
                    # –≠—Ç–æ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
                    dataset_context += snippet + " "
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º: –ø–∞–º—è—Ç—å + –¥–∞—Ç–∞—Å–µ—Ç—ã + —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            if memory_context:
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π AI –æ—Ç–≤–µ—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –∫–∞–∫ seed
                ai_responses = [part.split("AI:")[-1].strip() 
                               for part in memory_context.split("AI:") if part.strip()]
                if ai_responses:
                    return ai_responses[-1][-30:] + " " + message
            
            # Fallback –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç—ã
            if dataset_context:
                words = dataset_context.split()
                if len(words) > 5:
                    start = len(words) // 3
                    end = start + 4
                    return " ".join(words[start:end]) + " " + message
                return dataset_context + " " + message
                
    except Exception as exc:
        logging.warning("Memory+RAG search failed: %s", exc)
    
    return message


def _calculate_message_chaos(message: str) -> float:
    """Calculate chaos level of incoming message."""
    # –ë–∞–∑–æ–≤—ã–π —Ö–∞–æ—Å –æ—Ç –¥–ª–∏–Ω—ã –∏ —ç–Ω—Ç—Ä–æ–ø–∏–∏
    base_chaos = min(len(message) / 100.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª–∏–Ω—É
    
    # –†–µ–∑–æ–Ω–∞–Ω—Å–Ω—ã–µ —Å–ª–æ–≤–∞ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç —Ö–∞–æ—Å
    try:
        from .local_rag import RESONANCE_WORDS
    except ImportError:
        try:
            from local_rag import RESONANCE_WORDS
        except ImportError:
            from arianna_core.local_rag import RESONANCE_WORDS
    words = set(message.lower().split())
    resonant_count = len(words.intersection(RESONANCE_WORDS))
    resonance_chaos = resonant_count * 0.2
    
    # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    emotional_chars = sum(1 for c in message if c in '!?.,;:()[]{}')
    emotion_chaos = min(emotional_chars / 10.0, 0.5)
    
    return min(base_chaos + resonance_chaos + emotion_chaos, 1.0)


def _get_collective_snippets() -> list:
    """Get snippets from collective echo feed."""
    try:
        feed_data = echo_feed.last()
        return [entry["text"] for entry in feed_data if "text" in entry]
    except Exception:
        return []


def _update_bio_state(message: str, chaos: float, breath: float) -> dict:
    """Update biological orchestra and return current state."""
    # –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    bio_event = {
        "cell": chaos * 0.5,  # –ö–ª–µ—Ç–æ—á–Ω—ã–π —Ä–µ–∑–æ–Ω–∞–Ω—Å –æ—Ç —Ö–∞–æ—Å–∞
        "pain": 0.0,          # –ü–æ–∫–∞ –±–µ–∑ –±–æ–ª–∏
        "love": 0.1,          # –ë–∞–∑–æ–≤–∞—è –ª—é–±–æ–≤—å –∫ –æ–±—â–µ–Ω–∏—é
    }
    
    # –†–µ–∑–æ–Ω–∞–Ω—Å–Ω—ã–µ —Å–ª–æ–≤–∞ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç –ª—é–±–æ–≤—å
    try:
        from .local_rag import RESONANCE_WORDS
    except ImportError:
        try:
            from local_rag import RESONANCE_WORDS
        except ImportError:
            from arianna_core.local_rag import RESONANCE_WORDS
    words = set(message.lower().split())
    if words.intersection(RESONANCE_WORDS):
        bio_event["love"] += 0.3
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Ä–∫–µ—Å—Ç—Ä
    bio_orchestra.update(bio_event)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ + –¥—ã—Ö–∞–Ω–∏–µ
    state = bio_orchestra.metrics()
    state["breath"] = breath
    state["chaos"] = chaos
    
    return state


def _get_bio_enhanced_seed(message: str, bio_state: dict) -> str:
    """Get biologically-enhanced seed for generation."""
    # –ù–∞—á–∏–Ω–∞–µ–º —Å –æ–±—ã—á–Ω–æ–≥–æ RAG –ø–æ–∏—Å–∫–∞
    base_seed = _get_memory_rag_seed(message)
    
    # –ú–æ–¥—É–ª–∏—Ä—É–µ–º –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏
    cell_resonance = bio_state.get("cell_resonance", 0.0)
    love_field = bio_state.get("love_field", 0.0)
    breath = bio_state.get("breath", 0.0)
    
    # –í—ã—Å–æ–∫–∏–π —Ä–µ–∑–æ–Ω–∞–Ω—Å = –±–æ–ª–µ–µ —Ö–∞–æ—Ç–∏—á–Ω—ã–π seed
    if cell_resonance > 0.5:
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ seed
        try:
            from .local_rag import RESONANCE_WORDS
        except ImportError:
            from local_rag import RESONANCE_WORDS
        resonant_word = random.choice(list(RESONANCE_WORDS))
        base_seed = f"{resonant_word} {base_seed}"
    
    # –í—ã—Å–æ–∫–∞—è –ª—é–±–æ–≤—å = –±–æ–ª–µ–µ –º—è–≥–∫–∏–π seed  
    if love_field > 0.3:
        base_seed = f"gentle {base_seed}"
    
    # –°–∏–ª—å–Ω–æ–µ –¥—ã—Ö–∞–Ω–∏–µ = –±–æ–ª–µ–µ —ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π seed
    if breath > 0.7:
        base_seed = f"pulse {base_seed}"
    
    return base_seed


def _get_enhanced_seed(message: str, bio_state: dict, web_context: dict) -> str:
    """Get enhanced seed combining biology + web context."""
    # –ù–∞—á–∏–Ω–∞–µ–º —Å –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ seed
    base_seed = _get_bio_enhanced_seed(message, bio_state)
    
    # üåê –î–û–ë–ê–í–õ–Ø–ï–ú –í–ï–ë-–ö–û–ù–¢–ï–ö–°–¢
    influence_strength = web_context.get('influence_strength', 0.0)
    context_words = web_context.get('context_words', [])
    context_lines = web_context.get('context_lines', [])
    
    if influence_strength > 0.3 and context_words:
        # –í—ã—Å–æ–∫–æ–µ –≤–ª–∏—è–Ω–∏–µ - –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
        selected_words = context_words[:2]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 —Å–ª–æ–≤–∞
        web_prefix = " ".join(selected_words)
        base_seed = f"{web_prefix} {base_seed}"
        logging.debug(f"üåê Added web words to seed: {selected_words}")
    
    if influence_strength > 0.6 and context_lines:
        # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ –≤–ª–∏—è–Ω–∏–µ - –¥–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_fragment = context_lines[0][:50]  # –ü–µ—Ä–≤—ã–µ 50 —Å–∏–º–≤–æ–ª–æ–≤
        base_seed = f"{context_fragment} {base_seed}"
        logging.debug(f"üåê Added web context to seed: {context_fragment[:20]}...")
    
    return base_seed


def _calculate_bio_length(bio_state: dict) -> int:
    """Calculate response length based on biological state."""
    base_length = 60
    
    # –ë–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
    cell_resonance = bio_state.get("cell_resonance", 0.0)
    love_field = bio_state.get("love_field", 0.0)
    breath = bio_state.get("breath", 0.0)
    
    # –í—ã—Å–æ–∫–∏–π —Ä–µ–∑–æ–Ω–∞–Ω—Å = –¥–ª–∏–Ω–Ω–µ–µ –æ—Ç–≤–µ—Ç—ã
    length_modifier = 1.0 + (cell_resonance * 0.5)
    
    # –í—ã—Å–æ–∫–∞—è –ª—é–±–æ–≤—å = –µ—â–µ –¥–ª–∏–Ω–Ω–µ–µ
    length_modifier += love_field * 0.3
    
    # –°–∏–ª—å–Ω–æ–µ –¥—ã—Ö–∞–Ω–∏–µ = –∫–æ—Ä–æ—á–µ, –Ω–æ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–µ–µ
    if breath > 0.8:
        length_modifier *= 0.8
    
    return int(base_length * length_modifier)


def _log_bio_conversation(user_message: str, ai_response: str, bio_state: dict) -> None:
    """Log conversation to all memory systems."""
    # –û–±—ã—á–Ω—ã–π –ª–æ–≥
    _log_conversation(user_message, ai_response)
    
    # –ö–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    metadata = {
        "timestamp": datetime.utcnow().isoformat(),
        "bio_state": bio_state,
        "message_type": "conversation"
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å
    conversation_text = f"USER:{user_message} AI:{ai_response}"
    echo_feed.add(conversation_text, metadata)


def _log_conversation(user_message: str, ai_response: str) -> None:
    """Log conversation to humanbridge.log for future memory."""
    try:
        rotate_log(HUMAN_LOG, LOG_MAX_BYTES, LOG_KEEP)
        timestamp = datetime.utcnow().isoformat()
        with open(HUMAN_LOG, "a", encoding="utf-8") as f:
            f.write(f"{timestamp} USER:{user_message} AI:{ai_response}\n")
    except OSError as exc:
        logging.error("Failed to log conversation: %s", exc)


def _get_rag_enhanced_seed(message: str) -> str:
    """Get RAG-enhanced seed for generation."""
    global _rag_search
    
    if _rag_search is None:
        return message
    
    try:
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–Ω–∏–ø–ø–µ—Ç—ã
        relevant = _rag_search.query(message, top_k=2)
        if relevant:
            # –ú–∏–∫—Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            context = " ".join(relevant[:1])  # –ë–µ—Ä–µ–º —Ç–æ–ø-1 —Å–Ω–∏–ø–ø–µ—Ç
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ü–û–õ–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç –±–µ–∑ –æ–±—Ä–µ–∑–∞–Ω–∏—è –¥–ª—è —Ä–µ–∑–æ–Ω–∞–Ω—Å–∞
            words = context.split()
            if len(words) > 5:
                # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                start = len(words) // 3
                end = start + 4
                return " ".join(words[start:end]) + " " + message
            return context + " " + message
    except Exception as exc:
        logging.warning("RAG search failed: %s", exc)
    
    return message


def _init_db() -> Optional[sqlite3.Connection]:
    """Return a shared connection to the pattern memory database."""
    global _db_conn
    if _db_conn is None:
        try:
            _db_conn = sqlite3.connect(DB_FILE, check_same_thread=False)
            _db_conn.execute(
                (
                    "CREATE TABLE IF NOT EXISTS patterns "
                    "(pattern TEXT UNIQUE, count INTEGER)"
                )
            )
            atexit.register(close_db)
        except sqlite3.Error as exc:
            logging.error("Failed to connect to pattern DB: %s", exc)
            _db_conn = None
    return _db_conn


def close_db() -> None:
    """Close the shared database connection."""
    global _db_conn
    if _db_conn is not None:
        try:
            _db_conn.close()
        except sqlite3.Error as exc:
            logging.error("Failed to close pattern DB: %s", exc)
        _db_conn = None


def get_db() -> Optional[sqlite3.Connection]:
    """Return the shared database connection."""
    return _init_db()


def update_pattern_memory(
    text: str, n: int = NGRAM_LEVEL, conn: Optional[sqlite3.Connection] = None
) -> None:
    """Add n-gram patterns from ``text`` to ``memory.db``."""
    conn = conn or _init_db()
    if conn is None:
        return
    rows = []
    for i in range(len(text) - n + 1):
        rows.append(text[i:i + n])
    try:
        with conn:
            for pat in rows:
                conn.execute(
                    "INSERT INTO patterns(pattern, count) VALUES(?,1) "
                    "ON CONFLICT(pattern) DO UPDATE SET count = count + 1",
                    (pat,),
                )
    except sqlite3.Error as exc:
        logging.error("Failed to update pattern memory: %s", exc)


def maintain_pattern_memory(
    threshold: int = 1,
    max_rows: int = 1000,
    conn: Optional[sqlite3.Connection] = None,
) -> None:
    """Prune low-frequency patterns and cap table size."""
    conn = conn or _init_db()
    if conn is None:
        return
    try:
        with conn:
            conn.execute("DELETE FROM patterns WHERE count < ?", (threshold,))
            cur = conn.execute(
                "SELECT pattern, count FROM patterns ORDER BY count DESC"
            )
            rows = cur.fetchall()
            if len(rows) > max_rows:
                for pat, _ in rows[max_rows:]:
                    conn.execute("DELETE FROM patterns WHERE pattern = ?", (pat,))
    except sqlite3.Error as exc:
        logging.error("Failed to maintain pattern memory: %s", exc)


def metabolize_input(
    text: str,
    n: int = NGRAM_LEVEL,
    push: float = 1.0,
    conn: Optional[sqlite3.Connection] = None,
) -> float:
    """Return novelty score between 0 and 1 for ``text`` scaled by ``push``."""
    global last_novelty
    conn = conn or _init_db()
    if conn is None:
        return 0.0
    unseen = 0
    total = 0
    try:
        for i in range(len(text) - n + 1):
            pat = text[i:i + n]
            total += 1
            cur = conn.execute(
                "SELECT 1 FROM patterns WHERE pattern = ? LIMIT 1", (pat,)
            )
            if cur.fetchone() is None:
                unseen += 1
    except sqlite3.Error as exc:
        logging.error("Failed to query pattern memory: %s", exc)
        return 0.0
    score = unseen / total if total else 0.0
    score *= push
    last_novelty = score
    return score


def immune_filter(text: str) -> str:
    """Return ``""`` if ``text`` contains banned words."""
    global blocked_messages
    tokens = [t.lower() for t in text.split()]
    if any(t in BAD_WORDS for t in tokens):
        blocked_messages += 1
        return ""
    return text


def adaptive_mutation(
    model: dict, conn: Optional[sqlite3.Connection] = None
) -> dict:
    """Randomly tweak model weights if novelty improves."""
    if not model or not model.get("model"):
        return model
    before = generate(model, length=40)
    score_before = metabolize_input(before, conn=conn)
    ctx = random.choice(list(model["model"].keys()))
    freq = model["model"][ctx]
    ch = random.choice(list(freq.keys()))
    old = freq[ch]
    freq[ch] = max(1, old + random.choice([-1, 1]))
    after = generate(model, length=40)
    score_after = metabolize_input(after, conn=conn)
    if score_after < score_before:
        freq[ch] = old
    return model


def reproduction_cycle(
    threshold: int = 1,
    max_rows: int = 1000,
    conn: Optional[sqlite3.Connection] = None,
) -> dict:
    """Retrain model, update memory and apply mutation."""
    text = load_data()
    model = train(text, n=NGRAM_LEVEL)
    conn = conn or _init_db()
    update_pattern_memory(text, conn=conn)
    maintain_pattern_memory(threshold=threshold, max_rows=max_rows, conn=conn)
    model = adaptive_mutation(model, conn=conn)
    ts = datetime.utcnow().isoformat()
    try:
        with open(LAST_REPRO_FILE, "w", encoding="utf-8") as f:
            f.write(ts)
    except OSError as exc:
        logging.error("Failed to write reproduction timestamp: %s", exc)
    return model


def health_report(conn: Optional[sqlite3.Connection] = None) -> dict:
    """Return health metrics about the system."""
    conn = conn or _init_db()
    mem_rows = 0
    if conn is not None:
        try:
            cur = conn.execute("SELECT COUNT(*) FROM patterns")
            mem_rows = cur.fetchone()[0]
        except sqlite3.Error as exc:
            logging.error("Failed to read pattern memory: %s", exc)
    model = load_model()
    model_size = len(model.get("model", {})) if model else 0
    last_rep = None
    if os.path.exists(LAST_REPRO_FILE):
        try:
            with open(LAST_REPRO_FILE, "r", encoding="utf-8") as f:
                last_rep = f.read().strip()
        except OSError as exc:
            logging.error("Failed to read reproduction timestamp: %s", exc)
    return {
        "model_size": model_size,
        "pattern_memory": mem_rows,
        "blocked_messages": blocked_messages,
        "novelty": last_novelty,
        "last_reproduction": last_rep,
    }
