"""
Objectivity - –ú–∏–Ω–∏-–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞ –¥–ª—è Mini LE —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫

–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∏—â–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤ Mini LE.
–°–æ–∑–¥–∞–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–µ –æ–∫–Ω–æ –∑–Ω–∞–Ω–∏–π (5-10 —Å—Ç—Ä–æ–∫) –¥–ª—è –≤–ª–∏—è–Ω–∏—è –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é.
"""

import asyncio
import aiohttp
import os
import re
import random
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from urllib.parse import quote_plus
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Mini LE
SEARCH_TIMEOUT = 5  # —Å–µ–∫—É–Ω–¥ –Ω–∞ –ø–æ–∏—Å–∫
MAX_RESULTS = 3     # –º–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
CONTEXT_LINES = 8   # —Å—Ç—Ä–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è Mini LE (–º–µ–Ω—å—à–µ —á–µ–º —É LE)
LOG_DIR = Path("datasets")  # –∫—É–¥–∞ —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω–æ–µ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è


class ObjectivitySearch:
    """–ú–∏–Ω–∏-–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞ —á–µ—Ä–µ–∑ –≤–µ–±-–ø–æ–∏—Å–∫ –¥–ª—è Mini LE."""
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.search_engines = [
            self._search_duckduckgo,
            self._search_wikipedia_api,
            self._search_simple_google
        ]
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=SEARCH_TIMEOUT),
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def _search_duckduckgo(self, query: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo Instant Answer API."""
        try:
            url = f"https://api.duckduckgo.com/?q={quote_plus(query)}&format=json&no_html=1&skip_disambig=1"
            async with self.session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    results = []
                    
                    # –ê–±—Å—Ç—Ä–∞–∫—Ç
                    if data.get('Abstract'):
                        results.append(data['Abstract'])
                    
                    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                    if data.get('Definition'):
                        results.append(data['Definition'])
                    
                    # –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
                    for topic in data.get('RelatedTopics', [])[:2]:
                        if isinstance(topic, dict) and topic.get('Text'):
                            results.append(topic['Text'])
                    
                    return results[:3]
        except Exception as e:
            logging.debug(f"DuckDuckGo search failed: {e}")
        return []
    
    async def _search_wikipedia_api(self, query: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Wikipedia API."""
        try:
            # –ü–æ–∏—Å–∫ —Å—Ç—Ä–∞–Ω–∏—Ü
            search_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{quote_plus(query)}"
            async with self.session.get(search_url) as response:
                if response.status == 200:
                    data = await response.json()
                    if data.get('extract'):
                        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                        sentences = re.split(r'[.!?]+', data['extract'])
                        return [s.strip() for s in sentences[:3] if s.strip()]
        except Exception as e:
            logging.debug(f"Wikipedia search failed: {e}")
        return []
    
    async def _search_simple_google(self, query: str) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (fallback)."""
        # –≠—Ç–æ –∑–∞–≥–ª—É—à–∫–∞ - –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        return []
    
    def _extract_key_phrases(self, text_lines: List[str], user_query: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        all_text = " ".join(text_lines).lower()
        query_words = set(user_query.lower().split())
        
        # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        sentences = re.split(r'[.!?]+', all_text)
        relevant_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å –∑–∞–ø—Ä–æ—Å–æ–º
                sentence_words = set(sentence.split())
                if query_words & sentence_words:  # –ï—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
                    relevant_sentences.append(sentence)
        
        return relevant_sentences[:CONTEXT_LINES]
    
    def _calculate_influence_strength(self, context_lines: List[str], user_query: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–∏–ª—É –≤–ª–∏—è–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (0.0-1.0)."""
        if not context_lines:
            return 0.0
        
        query_words = set(user_query.lower().split())
        total_relevance = 0.0
        
        for line in context_lines:
            line_words = set(line.lower().split())
            # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
            intersection = query_words & line_words
            relevance = len(intersection) / max(len(query_words), 1)
            total_relevance += relevance
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        avg_relevance = total_relevance / len(context_lines)
        return min(avg_relevance * 2, 1.0)  # –£—Å–∏–ª–∏–≤–∞–µ–º –≤–ª–∏—è–Ω–∏–µ
    
    def _select_context_words(self, context_lines: List[str], count: int = 3) -> List[str]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≤–ª–∏—è–Ω–∏—è –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é."""
        all_words = []
        
        for line in context_lines:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
            words = re.findall(r'\b[a-zA-Z]{4,}\b', line)
            for word in words:
                word = word.lower()
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
                if word not in {'that', 'this', 'with', 'from', 'they', 'were', 'been', 'have', 'will', 'would', 'could', 'should'}:
                    all_words.append(word)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        unique_words = list(set(all_words))
        return random.sample(unique_words, min(count, len(unique_words)))
    
    async def search_context(self, user_query: str) -> Dict[str, any]:
        """
        –ò—â–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        
        Returns:
            {
                'context_lines': List[str],     # –°—Ç—Ä–æ–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                'influence_strength': float,    # –°–∏–ª–∞ –≤–ª–∏—è–Ω–∏—è (0.0-1.0)  
                'context_words': List[str],     # –°–ª–æ–≤–∞ –¥–ª—è –≤–ª–∏—è–Ω–∏—è –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                'found_sources': int            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            }
        """
        if not user_query.strip():
            return {
                'context_lines': [],
                'influence_strength': 0.0,
                'context_words': [],
                'found_sources': 0
            }
        
        all_results = []
        found_sources = 0
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏
        for search_func in self.search_engines:
            try:
                results = await search_func(user_query)
                if results:
                    all_results.extend(results)
                    found_sources += 1
            except Exception as e:
                logging.debug(f"Search engine failed: {e}")
        
        if not all_results:
            return {
                'context_lines': [],
                'influence_strength': 0.0,
                'context_words': [],
                'found_sources': 0
            }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
        context_lines = self._extract_key_phrases(all_results, user_query)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∏–ª—É –≤–ª–∏—è–Ω–∏—è
        influence_strength = self._calculate_influence_strength(context_lines, user_query)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –¥–ª—è –≤–ª–∏—è–Ω–∏—è
        context_words = self._select_context_words(context_lines)
        
        logging.info(f"üåê Objectivity: {len(context_lines)} lines, influence: {influence_strength:.2f}, words: {context_words}")
        
        return {
            'context_lines': context_lines[:CONTEXT_LINES],
            'influence_strength': influence_strength,
            'context_words': context_words,
            'found_sources': found_sources
        }
    
    def log_context_for_training(self, context_lines: List[str], user_query: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è Mini LE."""
        if not context_lines:
            return
        
        LOG_DIR.mkdir(exist_ok=True)
        log_file = LOG_DIR / "objectivity_context.txt"
        
        try:
            with log_file.open("a", encoding="utf-8") as f:
                f.write(f"# Query: {user_query}\n")
                for line in context_lines:
                    if line.strip():
                        f.write(f"{line.strip()}\n")
                f.write("\n")  # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        except Exception as e:
            logging.debug(f"Failed to log context: {e}")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
_search_instance: Optional[ObjectivitySearch] = None


async def search_objectivity(user_query: str) -> Dict[str, any]:
    """–£–¥–æ–±–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
    async with ObjectivitySearch() as searcher:
        result = await searcher.search_context(user_query)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
        if result['context_lines']:
            searcher.log_context_for_training(result['context_lines'], user_query)
        
        return result


def search_objectivity_sync(user_query: str) -> Dict[str, any]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Mini LE."""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # –ï—Å–ª–∏ —É–∂–µ –≤ event loop, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, search_objectivity(user_query))
                return future.result(timeout=SEARCH_TIMEOUT + 2)
        else:
            return asyncio.run(search_objectivity(user_query))
    except Exception as e:
        logging.debug(f"Objectivity search failed: {e}")
    return {
        'context_lines': [],
        'influence_strength': 0.0,
        'context_words': [],
        'found_sources': 0
    }


# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    async def test():
        async with ObjectivitySearch() as searcher:
            queries = [
                "what is consciousness",
                "artificial intelligence",
                "quantum physics",
                "love and meaning"
            ]
            
            for query in queries:
                print(f"\n{'='*50}")
                print(f"Testing query: '{query}'")
                result = await searcher.search_context(query)
                
                print(f"Influence strength: {result['influence_strength']:.2f}")
                print(f"Context words: {result['context_words']}")
                print("Context lines:")
                for i, line in enumerate(result['context_lines'][:5], 1):
                    print(f"  {i}. {line}")
    
    asyncio.run(test())
